{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE`/`raise NotImplementedError` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 02 Relational Databases: Project\n",
    "The goal of this project is to use a real-world relational database with a comprehensive dataset comprised of a significant number of tables and to combine our understanding of how to acquire data through SQL with our knowledge of Pandas and Tableau to investigate and explore the data to answer some interesting questions.\n",
    "\n",
    "## Relational Database Options\n",
    "\n",
    "Each team of one to three students can focus their efforts on one of two provided relational databases:\n",
    "\n",
    "1. `imdb` or `imdb2`: Movie/TV Series database\n",
    "2. `lahman2016`: Baseball statistics database\n",
    "\n",
    "### Movie Database\n",
    "\n",
    "- [Original `imdb` Data Tables Reference](https://docs.google.com/document/d/1fGhNj9_IvO5EqYt7w7v47jmfvnpUVFpARDtxBeO2HzA/edit?usp=sharing)\n",
    "\n",
    "As many of you know, IMDb is an online resource that maintains information on movies, tv, and celebrities for cinema produced from pretty much the beginning of movies up to the present.\n",
    "\n",
    "The link above gives the **data dictionary** that documents the tables and meaning of the columns within the tables for our original import of the data into our local MySQL server.  \n",
    "\n",
    "An improved version of the database (called `imdb2`) is also now available.  In that database, we apply many of our sound database design principles to get the database schema/architecture pictured above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"figures/imdbmodel.jpg\",width=\"1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a link to a prior student who, after the class, pursued an investigation of the IMDb data:\n",
    "\n",
    "http://thedataface.com/2018/06/culture/comedy-writing-staffs\n",
    "\n",
    "### Baseball Database\n",
    "\n",
    "[Data Tables Reference](https://docs.google.com/document/d/1Q6Mn2g_QT6u0_tcwRbXYp51eEhdzQceiochoI2GATEA/edit?usp=sharing)\n",
    "\n",
    "Sean Lahman, an award winning database journalist and author, has compiled the most comprehensive baseball database to date. The baseball archive has been active since 1995, making it the oldest running baseball website on the web. Every year Lahman makes updates to the Lahman Database, a free relational database version of the archive that covers the game of baseball back to 1871. Giving free access to detailed statistics starting just two years after the first professional baseball team was founded in Cincinnati in 1869, 30 years after the reported invention of the game. This database has over 25 relational tables which give novice and professional analysts a rich dataset.\n",
    "\n",
    "## Project Information\n",
    "\n",
    "As in the first project, I want you to synthesize the elements of what we have learned in this unit to ask interesting questions and use the data to create visualizations that help answer those questions.  In this case, we want to use the techniques of asking for data from multiple tables in a relational database.  We want to do so programmatically, and to define and use functions that provide good abstractions of the query operations we wish to perform.\n",
    "\n",
    "I am not prescriptive: you get to decide which database you are interested in using, and you get to be creative and postulate relationships between dependent and independent variables provided in the data and ask interesting questions that explore those relationships.  I am not asking for machine learning or advanced statistical methods to be employed ... there are plenty of interesting relationships that can be found directly by combining tables, grouping data in various ways, and building graphical visualizations of the results.\n",
    "\n",
    "### Required elements\n",
    "\n",
    "#### Processing\n",
    "\n",
    "- Need at least **three** SQL queries of moderate to high complexity that are issued to the database to get the data to answer your questions\n",
    "    - moderate to high complexity must involve multiple tables (two and three), and should also compute columns or do group-by and/or aggregation.\n",
    "    - I am assuming students working alone, but, if circumstances allow, you are welcome to have a team of two, and the number of SQL queries moves to **four**.\n",
    "- The \"heavy lifting\" must be done by the SQL, not by `pandas` subsequent manipulation, nor by work (joins, grouping) in `Tableau`\n",
    "- We **must** use good functional abstraction, which is best if we can include parameters that allow us to use the same function in more general ways through manipulation of the function arguments.\n",
    "    - I will be looking for functions **with parameters** that abstract each of your three queries.\n",
    "    - After a function that makes a query is called, there will be a function that handles writing the data results out to a csv for your visualization phase.\n",
    "    - There should be a `main()` function that controls the overall processing\n",
    "- The end result is a `processing.ipynb` that contains:\n",
    "    - your functions such that, when the notebook is run from start to finish, all queries get executed, and query results written to CSV files.\n",
    "\n",
    "#### Visualization\n",
    "\n",
    "Some basic requirements about your visualizations:\n",
    "\n",
    "- They should clearly answer questions about the data\n",
    "- They **must** have clear axis, labels, titles, and units, so that someone who did not generate the visualization can interpret it.\n",
    "   - Some may need values within the plotting area itself.\n",
    "- The end product is an essay in `report.ipynb`, targeted at a non-Python-SQL expert, describing the questions, the queries and strategies employed in solving the query problem with your SQL, and the visualizations and their interpretation.\n",
    "\n",
    "## Evaluation and Grading\n",
    "\n",
    "Your task is to write the Python and SQL code to acquire the data for each of your questions and analysis, writing (good, clean, coherent) functions to make your queries and bring the data into `Pandas` dataframes.  You should demonstrate the usability of the data by showing some of the data in your `processing.ipynb`.  The data should then be written to a csv for import into Tableau and the construction of your visualization.\n",
    "\n",
    "For this synthesis assignment, you may not, in fact, need tons of Python code.  So your Project assignments are not just about \"writing the program\".  I want you, through the Markdown and Notebook, as well as the visualization, to **clearly communicate** all of your steps in the start-to-finish progression and what you **learned about the datasets**.  As outlined in the rubric below, your grade is about all of these parts.\n",
    "\n",
    "I want to see you put together a coherent essay in an iPython Notebook that describes the steps taken, presents the code and rationale for why your functions coherently decompose and solve the overall problem, and uses graphs and English description to describe and **interpret** the results.  So the end of the notebook will be including the graphs/visualizations developed in Tableau and leading the reader through the interpretation.\n",
    "\n",
    "A good notebook should allow an audience/reader who is *not associated with this course* to understand the development and your conclusions.\n",
    "\n",
    "Your grade will be determined as follows:\n",
    "\n",
    "- 25 points for a well written and coherent essay and visualizations that answer good questions about the data\n",
    "- 20 points for well constructed, **documented**, explained SQL of moderate to high complexity for querying the database and doing the real work in getting the data to answer your questions\n",
    "- 15 points for the Python code, functional decomposition, code documentation\n",
    "\n",
    "As in all assignments, documentation for the **code** requires\n",
    "  - Complete docstrings for each function that include description, parameters, and return value.  Failing to include appropriate functions will result in the loss of associated docstring points.\n",
    "  - Comments in the code that describe (for a reader that might _only_ see your code) *how* the code is working.\n",
    "\n",
    "## Submission\n",
    "\n",
    "Your submission will consist of your two iPython Notebooks containing `processing` and `report` as described above.  You will also have multiple picture files (`.jpg`, `.png` etc.) showing the results of your visualization in Tableau.  Note that the picture files should be incorporated *in your notebook* as part of your documentation of the questions you asked about the dataset and how the visualizations help you in interpreting the data and answering those questions.\n",
    "\n",
    "Thus, you will upload into the course `Notebowl` the files:\n",
    "1. A `processing.ipynb` Notebook.\n",
    "2. A PDF of the `processing.ipynb` showing its execution from start to finish.\n",
    "3. A `report.ipynb` Notebook.\n",
    "4. A PDF of the `report.ipynb` showing its execution from start to finish.\n",
    "5. The set of `.jpg` or `.png` files for the figures generated in Tableau and included in your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Baseball Analysis\n",
    "\n",
    "Since Michael Lewis's book, **Moneyball**, published in 2003, that looked at the use of Data Analysis by certain teams to maximize their benefit relative to salary expense to assemble a better team for the money, interest in using similar techniques has flourished.  Through the Lahman database we have access to much of that same data.  In the following section, we will consider some of the following example graphs exploring baseball data that I found by searching the web.\n",
    "\n",
    "### Batting Average by Year\n",
    "\n",
    "The first example shows a scatter plot of Batting Average against year, to see the change over time, and to then plot some well known exceptional hitters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('figures/battingavgbyyear.jpg', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  grouping of individual batting averages for a given year might be filtered by players with at least a minimum number of times at bat.\n",
    "\n",
    "###  The Red Sox / Yankees Salary Story\n",
    "\n",
    "The next example was a simple visualization of cumulative salary for two teams over the years, focusing on the Boston Red Sox and the New York Yankees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('figures/bosox_v_nyy_salaries_through_years.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decreasing Variation in Players' Batting Averages\n",
    "\n",
    "The next analysis looks at the statistic of standard deviation in players' batting averages and plots this measure of variation as a function of time.  This decreasing trend is then used to argue the reason for the rarity of players who hit \"over 400\", (i.e greater than 40%) in modern times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('figures/sd-avg-good.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For inclusion in this analysis, there was again filtering against a certain number of opportunities at bat.\n",
    "\n",
    "### Salaries vs Wins\n",
    "\n",
    "The last example shows the effect of OAK in their moneyball efforts between 1997 and 2001, by building a scatter plot of wins against salary for the two years, and highlighting a couple of the teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('figures/salarieswinsera.jpg', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseball Analysis Tutorials/Examples\n",
    "\n",
    "The following itemized list provides references for some of the above examples:\n",
    "\n",
    "- [Introduction to Using R Packages for Baseball Research](https://www.fangraphs.com/tht/a-short-ish-introduction-to-using-r-for-baseball-research/)\n",
    "\n",
    "- [GitHub CS-109 Course Slides](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=9&cad=rja&uact=8&ved=0ahUKEwjC8omspeTWAhVI34MKHRo2Bd4QFghfMAg&url=https%3A%2F%2Fcs109.github.io%2F2014%2Fpages%2Flectures%2F03-hwkreview.html&usg=AOvVaw1OoEmN4tSoMtPOscVWLzch)\n",
    "\n",
    "- [Intro to Sabermetrics with Baseball Intro](http://adilmoujahid.com/posts/2014/07/baseball-analytics/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
