{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to Introduce Integration of SQL and Python Programming\n",
    "\n",
    "## Credentials Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells are the same as when we used cell magics for Database SQL interaction.  The result of the two cells is to compose a connection string in the variable `cstring`. This string is used to form a connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05ba6ed5fb91a26a5f262da397876fea",
     "grade": false,
     "grade_id": "cell-3c378fe37e86de2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def getCreds(filename, subset, defaults = {}):\n",
    "    \"\"\" Use `filename` to look for a file containing a json-encoded dictionary\n",
    "        of credentials.  If the file is successfully found and contains valid\n",
    "        json, return the sub-dictionary based on `subset`.  If the file is not\n",
    "        found, is not accessible, has improper encoding, or if the subset is \n",
    "        not present in the dictionary, return the given defaults.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            D = json.load(file)\n",
    "            if D[subset]:\n",
    "                return D[subset]\n",
    "            else:\n",
    "                return defaults\n",
    "    except:\n",
    "        return defaults\n",
    "\n",
    "creds = getCreds(\"creds.json\", \"mysql\", defaults={'user': 'studen_j1', \n",
    "                                                  'password': 'studen_j1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'mysql+mysqlconnector://{}:{}@hadoop2.mathsci.denison.edu/'\n",
    "cstring = template.format(creds['user'], creds['pass'])\n",
    "cstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module `sqlalchemy` Connection Establishment and First SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a connection string, the `sqlalchemy` module first creates a database \"engine\" that understands the SQL variation and the access library, as well as the other constituent parts of the connection string, which is really a database-specific URL.  In most instances, we will create a single connection to the database once we have created an engine, but, in general, the sqlalchemy abstraction allows the creation of *multiple* connections using the same engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(cstring)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplist form, a connection may be used as the channel over which we construct and send Python *strings*, whose contents consist of valid SQL.  In the case where we expect 0 rows as the result of an SQL, or where we are not interested in processing the result, we can just invoke with `execute` method of a connection with an argument of a Python string containing SQL.  In this particular example, we use SQL to set the default database currently being used by the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookDB = \"USE book\"\n",
    "connection.execute(bookDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss the `ResultProxy` more below, but for now, we issue a new SQL statement, one that retrieves all rows and all columns from the `indicators0` table, and we show the `fetchall()` method that, given the result of `execute()` ask for the ResultProxy to obtain all the rows satisfying the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getind = \"SELECT * FROM indicators0\"\n",
    "connection.execute(getind).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we get back a list of tuples, where each element of the list is a row of the result, and each tuple has the values for each of the columns in the table for that row.  If we wanted, we could assign this to a variable for further processing.\n",
    "\n",
    "We are next going to show a variation where the default database is specified at the time we create our sqlalchemy engine.  Before we do, we want to show the important step of, given an open connection and instance of an engine, how do we gracefully close down the connection and release the resources being used by the engine.  It is acomplished with the two steps in the cell below, assuming the name of the connection is `connection` and the name of the engine is `engine`.  If we had chosen other variables for the names of those objects when we created and assigned them, we would use those variable names instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()\n",
    "del engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can check that your connection is really closed. For example, if you go back and run the previous cell, to select data from `indicators0`, you will now get an error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Establishment and Basic Query II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we know, at the time we establish an engine and a connection, that we want to access and use a particular database.  The connection string URL allows for this possiblity by specifing the name of the database *after* the name of the server machine.  The code in the cell below, assuming the credentials dictionary setup from the second cell of this notebook, uses Python string `format` to create a connection string that includes the database name in addition to the user and password strings from the `creds` dictionary. Note that the string stored in the variable `database` will be brought into `cstring` as the third blank spot in `template` (at the end). Note that you could also get a value of `database` from `creds.json` instead of typing it into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'book'\n",
    "template = 'mysql+mysqlconnector://{}:{}@hadoop2.mathsci.denison.edu/{}'\n",
    "cstring = template.format(creds['user'], creds['pass'], database)\n",
    "cstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create the engine and connection as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(cstring)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can `execute()` an SQL statement without needing the `USE book` first.  In this case, we assign the return value of the `execute()` to the variable `resultproxy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getind = \"SELECT * FROM indicators0\"\n",
    "resultproxy = connection.execute(getind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Proxy Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of the return result from `execute()` is a `sqlalchemy.engine.result.ResultProxy`, an object class defined within `sqlalchemy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resultproxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ResultProxy is not yet the actuals rows resulting from the query, but we can find out, for instance, the column names of the result set by using the `keys()` method of a result proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultproxy.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a resultproxy, we can fetch rows of the result one at a time (using the `fetchone()` method), or as a batch (using the `fetchall()` method).  When we fetch one row, we get a special kind of tuple that has the values for all the columns from that row result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = resultproxy.fetchone()\n",
    "row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we invoke `fetchall()`, we get a **list of tuples**, one tuple per row.  Note that, because we previously fetched one row, the `fetchall()` starts in the result from whereever we \"left off\", in this case at the row corresponding to `CHN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = resultproxy.fetchall()\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `rows` variable is a list, we can perform normal list-type operations.  One such operation (available on any Python list), is the `insert()` operation, which given a mutable object like `rows`, and a first argument that specifies the index before which we want the insert to occur, and a second argumnet that specifies the item to insert, results in the desired update of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows.insert(0, row)\n",
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I stated that the rows of a query result were **special** tuples.  When we display such a row as an expression, or through a print, we get a normal-looking tuple.  But internally, these tuples each maintain the information about the column names of each of the tuple values.  If we invoke the `keys()` method on a **row**, we can get the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These special tuples also have an `items()` method, that allows us to iterate and get both the column name and the column value for each field in the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column, value in row.items():\n",
    "    template = \"column name: {:>10}, value: {}\"  # Note the format syntax to right justify in a field width of 10\n",
    "    s = template.format(column, value)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a DataFrame and Writing a Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often would prefer a `pandas` DataFrame, instead of a list of tuples, for working with the result of an SQL query.  From the `pandas` point of view, a list of tuples and a list of lists provide the same data representation of a list of rows containing the data.  Just like we did with a list of lists and then a list of column names, we can create a `pandas` DataFrame as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=resultproxy.keys())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is bad form to perform this set of operations in global cells.  By the principle of functional abstraction, we want an **abstraction** that retrieves the rows and columns of the Teams table.  The function should be parameterized by an `sqlalchemy` connection, since we should not reference a global variable in the body of our function.  Other than that, the sequence of statements in the body of the function look much like our global cell experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indicators(db_connection):\n",
    "    \"\"\" Retrieve the indicators0 table, given a live connection \n",
    "    to the database, as given in the `db_connection` parameter.\n",
    "        \n",
    "    Assumes that book is the current default database\n",
    "    set up on the connection.\n",
    "    \n",
    "    Returns a pandas dataframe with all rows and all columns \n",
    "    from the table.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the SQL query\n",
    "    getind = \"SELECT * FROM indicators0\"\n",
    "    \n",
    "    # Execute the query and fetch all rows of the result\n",
    "    resultproxy = db_connection.execute(getind)\n",
    "    resultset = resultproxy.fetchall()\n",
    "    \n",
    "    # Construct the pandas dataframe from the result\n",
    "    column_names = resultproxy.keys()\n",
    "    df = pd.DataFrame(resultset, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, whenever we want to get the `indicators0` table in a dataframe, we call our new function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_indicators(connection)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pandas` module actually understands `sqlalchemy` connections, and provides a module function that takes a Python string with an SQL query as its contents and builds a DataFrame from the result, as shown below.  Until further notice, however, since our goal is to understand the principles of working with a relational database from a programming language, the exclusive use of this shortcut is disallowed in homeworks and projects over the duration of this course unit. That is, you should still expect to have to solve problems as laid out above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query('SELECT * FROM indicators0', connection)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean up our connection and engine, in preparation for another setup example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close()\n",
    "del engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection Establishment III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can place the steps for setting up a connection and engine into its own function, so that we can reuse the code and can refrain from doing everything at the global cell level.  The parameters needed include the strings needed for the user, the password, and the default database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_setup(user, password, database):\n",
    "    template = 'mysql+mysqlconnector://{}:{}@hadoop2.mathsci.denison.edu/{}'\n",
    "    cstring = template.format(user, password, database)\n",
    "    e = sa.create_engine(cstring)\n",
    "    c = e.connect()\n",
    "    return e, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we need to make sure any previous engine and connection are cleaned up before we create new objects, the following cell attempts to close the connection and delete the engine.  This is placed in a Python `try`-`except` block, which will allow the code to succeed, even if there is not an open connections and an existant engine object.  So this cell is safe to \"repeat\"-execute whenever we want to establish or change the default database.\n",
    "\n",
    "In this case, we re-estblish a connection to the same database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    connection.close()\n",
    "    del engine\n",
    "except:\n",
    "    pass\n",
    "engine, connection = db_setup(creds['user'], creds['pass'], 'book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have an `engine` object, and it is associated with a default database, we can programmatically find out the set of tables present in the database.  This can be more convenient than using an external program like MySQL Workbench.  This is done with the `table_names()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query whether or not a particular table is present in the database with the `has_table()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.has_table('indicators0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.has_table('Tom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If are unsure of the column fields of a particular table, we can request a projection that includes all columns but, by virtue of a `LIMIT` requests no rows.  Even if we get no rows, we still get a result proxy, and the `keys()` method still works.  So below, I define a quick function that returns a list of column names for a given table name on a given database connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndColumns(conn, table):\n",
    "    sql_template = \"SELECT * FROM {} LIMIT 0\"\n",
    "    query = sql_template.format(table)\n",
    "    resultproxy = conn.execute(query)\n",
    "    return resultproxy.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getIndColumns(connection, \"indicators0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building SQL Queries that use values of Python variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When integrating programmatic problem solving with database access and queries, we need to incorporate values associated with Python variables in our programs with the SQL statements we are constructing before submitting them to the SQL server for execution.  In this section, we will explore a two mechanisms for accomplishing this:\n",
    "- Using a **variable binding** facility of the `sqlalchemy` moduleon the client side and generally supported by the various flavors of SQL servers.\n",
    "- Using basic facilities of Python string processing to incorporate the variable values into a composed string and then executing the string as we have above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binding Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we use a building block of the `text()` method of `sqlalchemy`'s `sql` class.  This allows us to use a Python string as an argument and construct an object capable of understanding SQL queries that include notation for **variable binding**, which will allow us to dynamically \"substitute\" values of variables at defined places within the query.  In the example below, the SQL syntax is the same as we have learned except for the `:x` and `:y` in the query string.  These define the locations for substitution and, in addition, give symbolic names (`x` and `y`) that enable us to distinguish when we have more than one substitution location within the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt = sa.sql.text(\"SELECT * FROM indicators0 WHERE life BETWEEN :x AND :y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the SQL text statement object above, we can make a call to `execute()` and use **named parameters**, where the parameter argument name matches the symbolic name given in the query string, and the value is whatever value we wish from our program, from constant literals, like shown below, to program variables, to arbitrary Python expressions yielding the desired value.  In the example below we are doing a combination of executing and **binding** of value `70` to `x` and `80` to `y` to get all countries whose life expectancy is between 70 and 80 (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.execute(stmt, x=70, y=80).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the variable `stmt` refers to an **unbound** query, we can reuse it later, with different binding to `x` and `y`, but without reconstructing `stmt`.  This promotes reusability of the so-called **paramaterized query** in multiple places in the program, and also helps readabilty, since we can see what parts of the query stay the same, and what parts may vary with arguments.\n",
    "\n",
    "Given an unbound query, we can explicitly make the step of binding values to the query parameters as shown in the cell below.  Note that we assign to a new variable, so that we can still have the unbound version of the query.  Also note, from the print, that the binding operation does not actually perform the subsitution within the query string.  Given a bound query, it is at the **server** that binding takes place.  This actually allows for better performing sequences of SQL queries, as the server can optimize and cache \"pre-compiled\" portions of the server execution based on the unbound portion.\n",
    "\n",
    "Our next example shows that how to bind string variables into a statement. This query finds all countries whose code starts with A,B,C, or D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmt2 = sa.sql.text(\"SELECT * FROM indicators0 WHERE code BETWEEN :x AND :y\")\n",
    "stmtbound = stmt2.bindparams(x=\"A\", y=\"Dz\")\n",
    "print(stmtbound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a bound version of a parameterized SQL query, we can invoke `execute()`, and omit the named parameters.  We can also, at that point, take advantage of the `pandas` `read_sql_query` function that can perform the execution and build a DataFrame on our behalf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(stmtbound, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable binding in our SQL queries can also give us a *clean* way to build functional abstractions that nicely generalize operations we wish to perform against a database.  Instead of using global variables and global cells to perform the above, we should always write a function.  In this case, since we want the function to take an arbitrary **\"from\"** and **\"to\"** to specify the range of names, we add parameters for each of these, and use the  techniques from above to compose a function that gets all the columns from the `indicators` table (not `indicators0`) for country codes in the range `namefrom` to `nameto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndicatorsInNameRange(connection, namefrom, nameto):\n",
    "    \"\"\" Retrieve a subset of the indicators table, \n",
    "    given a live connection to the database.\n",
    "    Assumes that book is the current default database\n",
    "    set up on the connection.\n",
    "    Returns a pandas dataframe with all columns from \n",
    "    the table, and whose codes are\n",
    "    in the inclusive range from `namefrom` to `nameto`\n",
    "    \"\"\"\n",
    "    getIndicators = sa.sql.text(\"SELECT * FROM indicators WHERE code BETWEEN :x AND :y\")\n",
    "    boundGetInd = getIndicators.bindparams(x=namefrom, y=nameto)\n",
    "    resultproxy = connection.execute(boundGetInd)\n",
    "    resultset = resultproxy.fetchall()\n",
    "    column_names = resultproxy.keys()\n",
    "    df = pd.DataFrame(resultset, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invocation of our new function can then be performed any time we want this functionality and can use different argments to specify different name ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getIndicatorsInNameRange(connection, \"H\", \"Kz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python string `format()` composition\n",
    "\n",
    "The same resultant effect can be obtained by using Python string manipulation to actually build the complete query string, with programmatic techniques used to take the values from our Python program and substitute them into the string to form the query.  This complete query string can then be sent and executed at the server.\n",
    "\n",
    "In the following cell, we show how we can use the Python string `format()` method, which substitutes values from the arguments to format into a template string.  The location of the substitution/conversion is specified using curly braces `{}` in the template string.  There is an intentional error in this code.  Execute the definition cell and then the invocation cell and see if you can figure out the problem and correct it. Hint: look at the very bottom of the error message and notice what's wrong with that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndicatorsInNameRange2(connection, namefrom, nameto):\n",
    "    \"\"\" Retrieve a subset of the indicators table, \n",
    "    given a live connection to the database.\n",
    "    Assumes that book is the current default database\n",
    "    set up on the connection.\n",
    "    Returns a pandas dataframe with all columns from \n",
    "    the table, and whose codes are\n",
    "    in the inclusive range from `namefrom` to `nameto`\n",
    "    \"\"\"\n",
    "    getIndicators = \"SELECT * FROM indicators WHERE code BETWEEN {} AND {}\"\n",
    "    formatGetInd = getIndicators.format(namefrom, nameto)\n",
    "    resultproxy = connection.execute(formatGetInd)\n",
    "    resultset = resultproxy.fetchall()\n",
    "    column_names = resultproxy.keys()\n",
    "    df = pd.DataFrame(resultset, columns=column_names)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getIndicatorsInNameRange2(connection, \"H\", \"Kz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a similar example, where we want to specify a range of integers to use to get countries with `life` in a specified range. The first function uses the SQL bind variables approach, and it looks much like the function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLifeInRange(connection, lifefrom, lifeto):\n",
    "    \"\"\" Retrieve a subset of the indicators table, given \n",
    "    a live connection to the database.\n",
    "    Assumes that book is the current default database\n",
    "    set up on the connection.\n",
    "    Returns a pandas dataframe with all columns from the \n",
    "    table, and whose life expectancies are\n",
    "    in the inclusive range from `lifefrom` to `lifeto`\n",
    "    \"\"\"\n",
    "    getCountries = sa.sql.text(\"SELECT * FROM indicators WHERE life BETWEEN :x AND :y\")\n",
    "    boundgetCountries = getCountries.bindparams(x=lifefrom, y=lifeto)\n",
    "    resultproxy = connection.execute(boundgetCountries)\n",
    "    resultset = resultproxy.fetchall()\n",
    "    column_names = resultproxy.keys()\n",
    "    df = pd.DataFrame(resultset, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLifeInRange(connection, 70, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we repeat the exercise from above -- namely using Python string manipulation to create a string query without bind variables and execute it.  Note that this version is just like the uncorrected `getIndicatorsInNameRange2()` function above.  What happens if you execute this without change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndicatorsInNameRange2(connection, lifefrom, lifeto):\n",
    "    \"\"\" Retrieve a subset of the indicators table, given \n",
    "    a live connection to the database.\n",
    "    Assumes that book is the current default database\n",
    "    set up on the connection.\n",
    "    Returns a pandas dataframe with all columns from the \n",
    "    table, and whose life expectancies are\n",
    "    in the inclusive range from `lifefrom` to `lifeto`\n",
    "    \"\"\"\n",
    "    getCountries = \"SELECT * FROM indicators WHERE life BETWEEN {} AND {}\"\n",
    "    formatGetCountries = getCountries.format(lifefrom, lifeto)\n",
    "    resultproxy = connection.execute(formatGetCountries)\n",
    "    resultset = resultproxy.fetchall()\n",
    "    column_names = resultproxy.keys()\n",
    "    df = pd.DataFrame(resultset, columns=column_names)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getIndicatorsInNameRange2(connection, 70, 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make the same change you made to correct `getIndicatorsInNameRange2()` in `getLifeInRange2()` above and reexecute.  What results do you get?\n",
    "\n",
    "The point of this exercise is to emphasize that **data types matter.** . If we have string data types in an SQL query, we have to make sure a variable substituted into a query has the single quotes surrounding it, if we are doing the string format method.  If we are using bind variables, the underlying library/software (both on the client side and on the server side) understands the data type of the variables being bound, and accommodates appropriately.  This is one factor in favor of using bind variables.\n",
    "\n",
    "The last example shows a combination of string processing along with still using bind variables.  Say that you want a functional abstraction that paramerizes the life expectancy range to select from the `indicators` table, but sometimes you want all columns projected, and other times you want a subset of the columns.  To have a single function that enables both, we can add a (string) parameter that abstracts the desired columns.  In the function definition, we can have a default of `*` to select all columns.  Then, if we do not include an argument for `cols`, we get the same behavior as before.  But if we specify a `cols` argument, the function first uses string manipulation (the `.format()` method) to compose the query string, including the variable bindings.  We then proceed the same way we did in the previous bind variables case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLifeInRange3(connection, lifefrom, lifeto, cols='*'):\n",
    "    \"\"\" Retrieve a subset of the indicators table, given a live connection to the database.\n",
    "    Assumes that book is the current default database\n",
    "    set up on the connection.\n",
    "    Returns a pandas dataframe with all columns from the table, \n",
    "    and whose life expectancies are\n",
    "    in the inclusive range from `lifefrom` to `lifeto`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the SQL Query\n",
    "    getLife0 = \"SELECT {} FROM indicators WHERE life BETWEEN :x AND :y\".format(cols)\n",
    "    getLife = sa.sql.text(getLife0)\n",
    "    boundGetLife = getLife.bindparams(x=lifefrom, y=lifeto)\n",
    "    \n",
    "    # Execute and fetch all rows of the result\n",
    "    resultproxy = connection.execute(boundGetLife)\n",
    "    resultset = resultproxy.fetchall()\n",
    "    \n",
    "    # Build a pandas dataframe using the result\n",
    "    column_names = resultproxy.keys()\n",
    "    df = pd.DataFrame(resultset, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLifeInRange3(connection, 60, 65, \"code, year, life\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default value for `cols` in the function above is `*` meaning if you leave off the variable `cols` when calling `getLifeInRange3` then it'll select all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLifeInRange3(connection, 60, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
